import os
import json
from collections import defaultdict

ROOT_DIR = "nier_text_json"
LOG_FILE = "remove_duplicates_log.txt"

def collect_json_files(root_dir):
    json_files = []
    for root, dirs, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".json"):
                json_files.append(os.path.join(root, file))
    return json_files

def load_all_entries(json_files):
    all_entries = []
    for file_path in json_files:
        with open(file_path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                for entry in data:
                    key = (entry.get("id"), entry.get("en"))
                    all_entries.append((key, entry, file_path))
            except json.JSONDecodeError as e:
                print(f"–û—à–∏–±–∫–∞ JSON –≤ —Ñ–∞–π–ª–µ: {file_path} ‚Äî {e}")
    return all_entries

def deduplicate_entries(all_entries):
    unique_entries = {}
    duplicates_info = []

    for key, entry, file_path in all_entries:
        if key in unique_entries:
            existing = unique_entries[key]
            en_voice_before = existing["entry"].get("en_voice")
            en_voice_new = entry.get("en_voice")

            replaced_voice = False
            if (not en_voice_before) and en_voice_new:
                existing["entry"]["en_voice"] = en_voice_new
                replaced_voice = True

            duplicates_info.append({
                "id": key[0],
                "en": key[1],
                "kept_file": existing["file"],
                "duplicate_file": file_path,
                "en_voice_replaced": replaced_voice
            })
        else:
            unique_entries[key] = {"entry": entry, "file": file_path}
    return unique_entries, duplicates_info

def write_back_files(unique_entries, json_files):
    new_file_data = defaultdict(list)
    for info in unique_entries.values():
        new_file_data[info["file"]].append(info["entry"])

    for file_path in json_files:
        data = new_file_data.get(file_path, [])
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

def write_log(duplicates_info, log_file_path):
    with open(log_file_path, "w", encoding="utf-8") as log:
        for dup in duplicates_info:
            log.write(f"üîÅ –ü–æ–≤—Ç–æ—Ä:\n")
            log.write(f"  ID: {dup['id']}\n")
            log.write(f"  EN: {dup['en']}\n")
            log.write(f"  ‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω: {dup['kept_file']}\n")
            log.write(f"  ‚ùå –£–¥–∞–ª—ë–Ω:   {dup['duplicate_file']}\n")
            if dup["en_voice_replaced"]:
                log.write(f"  ‚ö†Ô∏è  en_voice –±—ã–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –∏–∑ —É–¥–∞–ª—ë–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏.\n")
            log.write("\n")

def main():
    print("üîç –ò—â–µ–º JSON-—Ñ–∞–π–ª—ã...")
    json_files = collect_json_files(ROOT_DIR)
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(json_files)}")

    print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    all_entries = load_all_entries(json_files)
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(all_entries)}")

    print("üßπ –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—ã...")
    unique_entries, duplicates_info = deduplicate_entries(all_entries)
    print(f"–û—Å—Ç–∞–≤–ª–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(unique_entries)}")
    print(f"–£–¥–∞–ª–µ–Ω–æ –ø–æ–≤—Ç–æ—Ä–æ–≤: {len(duplicates_info)}")

    print("üíæ –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º JSON-—Ñ–∞–π–ª—ã...")
    write_back_files(unique_entries, json_files)

    print(f"üìù –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –ª–æ–≥ –≤ {LOG_FILE} ...")
    write_log(duplicates_info, LOG_FILE)

    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")

if __name__ == "__main__":
    main()
